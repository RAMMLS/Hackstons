from flask import Flask, render_template, request, jsonify
import requests
import whois
from bs4 import BeautifulSoup
import urllib.parse
from datetime import datetime
import ssl

app = Flask(__name__)

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É SSL –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ—à–∏–±–æ–∫ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤
ssl._create_default_https_context = ssl._create_unverified_context

@app.route('/')
def home():
    return render_template('index.html')

# @app.route('/analyze', methods=['POST'])
# def analyze_url():
#     try:
#         data = request.get_json()
#         url = data.get('url', '').strip()
        
#         if not url:
#             return jsonify({'error': 'URL –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω'}), 400
        
#         # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ö–µ–º—É –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
#         if not url.startswith(('http://', 'https://')):
#             url = 'https://' + url
        
#         print(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º URL: {url}")
        
#         # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
#         analysis_result = perform_comprehensive_analysis(url)
        
#         return jsonify(analysis_result)
        
#     except Exception as e:
#         print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")
#         return jsonify({'error': f'–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}'}), 500

@app.route('/analyze', methods=['POST'])
def analyze_url():
    try:
        data = request.get_json()
        url = data.get('url', '').strip()
        
        if not url:
            return jsonify({'error': 'URL –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω'}), 400
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ö–µ–º—É –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        print(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ URL: {url}")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
        print("üìù –í—ã–∑—ã–≤–∞–µ–º perform_comprehensive_analysis...")
        analysis_result = perform_comprehensive_analysis(url)
        print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {analysis_result}")
        
        return jsonify(analysis_result)
        
    except Exception as e:
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ analyze_url: {str(e)}")
        import traceback
        print(f"üîç TRACEBACK: {traceback.format_exc()}")
        return jsonify({'error': f'–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}'}), 500

# def perform_comprehensive_analysis(url):
#     """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ URL"""
#     result = {
#         'url': url,
#         'timestamp': datetime.now().isoformat(),
#         'checks': {},
#         'final_verdict': {}
#     }
    
#     # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
#     print("üì° –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å...")
#     result['checks']['accessibility'] = check_accessibility(url)
    
#     # 2. WHOIS –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
#     print("üåê –ü–æ–ª—É—á–∞–µ–º WHOIS –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é...")
#     result['checks']['whois'] = check_whois_info(url)
    
#     # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ robots.txt
#     print("ü§ñ –ü—Ä–æ–≤–µ—Ä—è–µ–º robots.txt...")
#     result['checks']['robots'] = check_robots_txt(url)
    
#     # 4. –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
#     print("üìÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å–∞–π—Ç–∞...")
#     result['checks']['content'] = analyze_website_content(url)
    
#     # 5. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
#     print("üéØ –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é OSINT/CSINT...")
#     result['final_verdict'] = determine_category(result['checks'])
    
#     print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
#     return result

# def perform_comprehensive_analysis(url):
#     """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ URL"""
#     try:
#         result = {
#             'url': url,
#             'timestamp': datetime.now().isoformat(),
#             'checks': {},
#             'final_verdict': {}
#         }
        
#         # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
#         print("üì° –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å...")
#         result['checks']['accessibility'] = check_accessibility(url)
        
#         # 2. WHOIS –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–º–æ–∂–µ—Ç –≤—ã–∑—ã–≤–∞—Ç—å –æ—à–∏–±–∫–∏)
#         print("üåê –ü–æ–ª—É—á–∞–µ–º WHOIS –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é...")
#         try:
#             result['checks']['whois'] = check_whois_info(url)
#         except Exception as e:
#             result['checks']['whois'] = {'error': f'WHOIS failed: {str(e)}'}
        
#         # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ robots.txt
#         print("ü§ñ –ü—Ä–æ–≤–µ—Ä—è–µ–º robots.txt...")
#         try:
#             result['checks']['robots'] = check_robots_txt(url)
#         except Exception as e:
#             result['checks']['robots'] = {'error': f'Robots failed: {str(e)}'}
        
#         # 4. –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
#         print("üìÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å–∞–π—Ç–∞...")
#         try:
#             result['checks']['content'] = analyze_website_content(url)
#         except Exception as e:
#             result['checks']['content'] = {'error': f'Content analysis failed: {str(e)}'}
        
#         # 5. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
#         print("üéØ –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é OSINT/CSINT...")
#         result['final_verdict'] = determine_category(result['checks'])
        
#         print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
#         return result
        
#     except Exception as e:
#         print(f"‚ùå CRITICAL ERROR in analysis: {e}")
#         return {
#             'url': url,
#             'error': str(e),
#             'final_verdict': {
#                 'category': 'ERROR',
#                 'confidence': 0,
#                 'category_reason': '–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ'
#             }
#         }

def perform_comprehensive_analysis(url):
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ URL"""
    try:
        result = {
            'url': url,
            'timestamp': datetime.now().isoformat(),
            'checks': {},
            'final_verdict': {}
        }
        
        # –¢–û–õ–¨–ö–û –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ (—Å–∞–º–∞—è –Ω–∞–¥–µ–∂–Ω–∞—è)
        print("üì° –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å...")
        result['checks']['accessibility'] = check_accessibility(url)
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º WHOIS –∏ robots.txt –≤—Ä–µ–º–µ–Ω–Ω–æ
        result['checks']['whois'] = {'status': 'temporarily_disabled'}
        result['checks']['robots'] = {'status': 'temporarily_disabled'}
        
        # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        print("üìÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å–∞–π—Ç–∞...")
        try:
            result['checks']['content'] = analyze_website_content(url)
        except Exception as e:
            result['checks']['content'] = {'error': f'Content analysis failed: {str(e)}'}
        
        # –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        print("üéØ –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é OSINT/CSINT...")
        result['final_verdict'] = determine_category_simple(result['checks'])
        
        print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        return result
        
    except Exception as e:
        print(f"‚ùå CRITICAL ERROR: {e}")
        return {
            'url': url,
            'error': str(e),
            'final_verdict': {
                'category': 'ERROR',
                'confidence': 0,
                'category_reason': '–í—Ä–µ–º–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞'
            }
        }

def determine_category_simple(checks):
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    accessibility = checks.get('accessibility', {})
    
    if accessibility.get('http_status') == 200:
        return {
            'category': 'OSINT',
            'confidence': 85,
            'category_reason': '–°–∞–π—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ø—É–±–ª–∏—á–Ω–æ',
            'reasons': ['‚úÖ –°–∞–π—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ø—É–±–ª–∏—á–Ω–æ (HTTP 200)']
        }
    else:
        return {
            'category': 'CSINT',
            'confidence': 70,
            'category_reason': '–ü—Ä–æ–±–ª–µ–º—ã —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ —Å–∞–π—Ç—É',
            'reasons': ['‚ùå –°–∞–π—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ —Ç—Ä–µ–±—É–µ—Ç –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏']
        }


def check_accessibility(url):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–∞–π—Ç–∞"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        print(f"   ‚Üí –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ {url}")
        response = requests.get(url, headers=headers, timeout=10, allow_redirects=True)
        
        return {
            'http_status': response.status_code,
            'final_url': response.url,
            'requires_auth': response.status_code in [401, 403],
            'is_redirected': response.history != [],
            'access_time': response.elapsed.total_seconds(),
            'accessible': True
        }
    except requests.exceptions.RequestException as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞: {e}")
        return {
            'http_status': None,
            'error': str(e),
            'accessible': False
        }

# def check_whois_info(url):
#     """–ü–æ–ª—É—á–µ–Ω–∏–µ WHOIS –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
#     try:
#         from urllib.parse import urlparse
#         domain = urlparse(url).netloc
        
#         # –£–±–∏—Ä–∞–µ–º www. –µ—Å–ª–∏ –µ—Å—Ç—å
#         if domain.startswith('www.'):
#             domain = domain[4:]
            
#         print(f"   ‚Üí –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º WHOIS –¥–ª—è: {domain}")
#         whois_data = whois.whois(domain)
        
#         return {
#             'domain_name': str(whois_data.domain_name) if whois_data.domain_name else None,
#             'registrar': whois_data.registrar,
#             'creation_date': str(whois_data.creation_date) if whois_data.creation_date else None,
#             'expiration_date': str(whois_data.expiration_date) if whois_data.expiration_date else None,
#             'name_servers': list(whois_data.name_servers) if whois_data.name_servers else None,
#             'emails': whois_data.emails,
#             'is_private': whois_data.private if hasattr(whois_data, 'private') else False
#         }
#     except Exception as e:
#         print(f"   ‚ùå –û—à–∏–±–∫–∞ WHOIS: {e}")
#         return {'error': f'WHOIS –æ—à–∏–±–∫–∞: {str(e)}'}

def check_whois_info(url):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ WHOIS –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    try:
        from urllib.parse import urlparse
        domain = urlparse(url).netloc
        
        # –£–±–∏—Ä–∞–µ–º www. –µ—Å–ª–∏ –µ—Å—Ç—å
        if domain.startswith('www.'):
            domain = domain[4:]
            
        print(f"   ‚Üí –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º WHOIS –¥–ª—è: {domain}")
        whois_data = whois.whois(domain)
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ —Å–ª–æ–∂–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        return {
            'domain_name': str(domain),
            'registrar': getattr(whois_data, 'registrar', 'Unknown'),
            'is_private': getattr(whois_data, 'private', False)
        }
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ WHOIS: {e}")
        return {'error': f'WHOIS unavailable: {str(e)}'}
    
def check_robots_txt(url):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ robots.txt"""
    try:
        from urllib.parse import urlparse
        parsed_url = urlparse(url)
        robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"
        
        print(f"   ‚Üí –ü—Ä–æ–≤–µ—Ä—è–µ–º: {robots_url}")
        response = requests.get(robots_url, timeout=5)
        
        if response.status_code == 200:
            allows_crawling = 'User-agent: *' in response.text and 'Disallow: /' not in response.text
            return {
                'exists': True,
                'content_preview': response.text[:500],  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤
                'allows_crawling': allows_crawling
            }
        else:
            return {'exists': False}
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ robots.txt: {e}")
        return {'exists': False, 'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å robots.txt'}

def analyze_website_content(url):
    """–ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å–∞–π—Ç–∞"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        print(f"   ‚Üí –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç...")
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # –ü–æ–∏—Å–∫ —Ñ–æ—Ä–º –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
        login_forms = soup.find_all('form')
        has_login_form = any(
            form.find('input', {'type': 'password'}) or 
            'login' in form.get('action', '').lower() or
            'signin' in form.get('action', '').lower() or
            'auth' in form.get('action', '').lower()
            for form in login_forms
        )
        
        # –ü–æ–∏—Å–∫ paywall –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        text = soup.get_text().lower()
        has_paywall_indicators = any(
            indicator in text for indicator in 
            ['subscribe', 'premium', 'membership', 'paywall', 'subscription', 'pay to read']
        )
        
        return {
            'title': soup.title.string if soup.title else 'No title',
            'has_login_form': has_login_form,
            'has_paywall_indicators': has_paywall_indicators,
            'form_count': len(login_forms),
            'meta_description': soup.find('meta', attrs={'name': 'description'})
        }
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
        return {'error': f'–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {str(e)}'}

def determine_category(checks):
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ OSINT/CSINT –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≤–µ—Ä–æ–∫"""
    score = 0
    reasons = []
    
    print("   ‚Üí –í—ã—á–∏—Å–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é...")
    
    # –ö—Ä–∏—Ç–µ—Ä–∏–π 1: –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å (30 –±–∞–ª–ª–æ–≤)
    accessibility = checks.get('accessibility', {})
    if accessibility.get('http_status') == 200:
        score += 30
        reasons.append("‚úÖ –°–∞–π—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ø—É–±–ª–∏—á–Ω–æ (HTTP 200)")
    elif accessibility.get('requires_auth'):
        score -= 20
        reasons.append("‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è")
    elif not accessibility.get('accessible', False):
        score -= 15
        reasons.append("‚ùå –°–∞–π—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    # –ö—Ä–∏—Ç–µ—Ä–∏–π 2: WHOIS –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (25 –±–∞–ª–ª–æ–≤)
    whois_info = checks.get('whois', {})
    if not whois_info.get('error'):
        if not whois_info.get('is_private', True):
            score += 25
            reasons.append("‚úÖ –ü—É–±–ª–∏—á–Ω–∞—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞")
        else:
            score -= 10
            reasons.append("‚ö†Ô∏è –ü—Ä–∏–≤–∞—Ç–Ω–∞—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞")
    else:
        score -= 5
        reasons.append("‚ö†Ô∏è WHOIS –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
    
    # –ö—Ä–∏—Ç–µ—Ä–∏–π 3: Robots.txt (15 –±–∞–ª–ª–æ–≤)
    robots = checks.get('robots', {})
    if robots.get('exists'):
        if robots.get('allows_crawling', False):
            score += 15
            reasons.append("‚úÖ –†–∞–∑—Ä–µ—à–µ–Ω –∫—Ä–∞—É–ª–∏–Ω–≥ –≤ robots.txt")
        else:
            score -= 5
            reasons.append("‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ robots.txt")
    else:
        score += 5
        reasons.append("‚ÑπÔ∏è Robots.txt –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    # –ö—Ä–∏—Ç–µ—Ä–∏–π 4: –ö–æ–Ω—Ç–µ–Ω—Ç –∞–Ω–∞–ª–∏–∑ (30 –±–∞–ª–ª–æ–≤)
    content = checks.get('content', {})
    if not content.get('error'):
        if not content.get('has_login_form', False):
            score += 15
            reasons.append("‚úÖ –ù–µ—Ç —Ñ–æ—Ä–º –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏")
        else:
            score -= 10
            reasons.append("‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ñ–æ—Ä–º—ã –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏")
        
        if not content.get('has_paywall_indicators', False):
            score += 15
            reasons.append("‚úÖ –ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ paywall")
        else:
            score -= 10
            reasons.append("‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ paywall")
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    confidence = max(0, min(100, score))
    
    if confidence >= 70:
        category = "OSINT"
        category_reason = "–ò—Å—Ç–æ—á–Ω–∏–∫ —è–≤–ª—è–µ—Ç—Å—è –æ—Ç–∫—Ä—ã—Ç—ã–º –∏ –ø—É–±–ª–∏—á–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã–º"
    elif confidence >= 40:
        category = "POTENTIALLY_OSINT"
        category_reason = "–ò—Å—Ç–æ—á–Ω–∏–∫ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –æ—Ç–∫—Ä—ã—Ç, –Ω–æ –µ—Å—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è"
    else:
        category = "CSINT"
        category_reason = "–ò—Å—Ç–æ—á–Ω–∏–∫ –∏–º–µ–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–∞"
    
    print(f"   ‚Üí –†–µ–∑—É–ª—å—Ç–∞—Ç: {category} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence}%)")
    
    return {
        'category': category,
        'confidence': confidence,
        'score_breakdown': score,
        'reasons': reasons,
        'category_reason': category_reason
    }

if __name__ == '__main__':
    print("üöÄ –ó–∞–ø—É—Å–∫ –ü–û–õ–ù–û–ì–û —Å–µ—Ä–≤–µ—Ä–∞ –∞–Ω–∞–ª–∏–∑–∞ OSINT/CSINT...")
    print("üì° –ê–¥—Ä–µ—Å: http://127.0.0.1:5000")
    print("üîß –†–µ–∂–∏–º: –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å WHOIS –∏ –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    print("--------------------------------------------------")
    app.run(debug=True)
